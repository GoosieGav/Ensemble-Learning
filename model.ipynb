{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Sklearn modules\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "# TensorFlow/Keras modules\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks, regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Machine Learning models\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import optuna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Available devices:\", tf.config.list_physical_devices())\n",
    "print(\"Is GPU available?\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/malmem2024-osef.csv')\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_malware(category):\n",
    "    if category.startswith(\"Ransomware\"):\n",
    "        return \"Ransomware\"\n",
    "    elif category.startswith(\"Trojan\"):\n",
    "        return \"Trojan\"\n",
    "    elif category.startswith(\"Spyware\"):\n",
    "        return \"Spyware\"\n",
    "    elif category == \"Benign\":\n",
    "        return \"Benign\"\n",
    "    else:\n",
    "        return \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"GeneralCategory\"] = df[\"Category\"].apply(categorize_malware)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "df[\"EncodedCategory\"] = label_encoder.fit_transform(df[\"GeneralCategory\"])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "print(\"Label Encoding Mapping:\", class_mapping)  # Example: {'Benign': 0, 'Ransomware': 1, 'Spyware': 2, 'Trojan': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"Category\", \"GeneralCategory\", \"EncodedCategory\", \"Class\"])  # Drop target columns\n",
    "y = df[\"EncodedCategory\"]  # Use the encoded labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop features where all values are the same\n",
    "X_filtered = X.loc[:, X.nunique() > 1]\n",
    "\n",
    "print(f\"Removed {X.shape[1] - X_filtered.shape[1]} constant-value features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_distribution = df[\"EncodedCategory\"].value_counts()\n",
    "print(class_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_filtered, y, test_size=0.2, stratify=y, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "sns.barplot(x=y_train.value_counts().index, y=y_train.value_counts().values, palette=\"coolwarm\")\n",
    "plt.xlabel(\"Malware Class\")\n",
    "plt.ylabel(\"Number of Samples\")\n",
    "plt.title(\"Class Distribution Before SMOTE\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "sns.barplot(x=y_train_resampled.value_counts().index, y=y_train_resampled.value_counts().values, palette=\"coolwarm\")\n",
    "plt.xlabel(\"Malware Class\")\n",
    "plt.ylabel(\"Number of Samples\")\n",
    "plt.title(\"Class Distribution After SMOTE\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_distribution = y_train_resampled.value_counts()\n",
    "print(class_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = X_train_resampled.shape[1]\n",
    "print(f\"Number of features: {num_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 500),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 15),\n",
    "        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.001, 1.0),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0, 0.3),\n",
    "        \"reg_alpha\": trial.suggest_loguniform(\"reg_alpha\", 1e-5, 1e-1),  # L1 regularization\n",
    "        \"reg_lambda\": trial.suggest_loguniform(\"reg_lambda\", 1e-5, 1e-1),  # L2 regularization\n",
    "    }\n",
    "\n",
    "    # Create XGBoost model\n",
    "    model = xgb.XGBClassifier(**params, use_label_encoder=False, eval_metric=\"mlogloss\", random_state=42)\n",
    "\n",
    "    # Cross-validation score\n",
    "    scores = cross_val_score(model, X_train_resampled, y_train_resampled, cv=3, scoring='accuracy')\n",
    "\n",
    "    return scores.mean()  # Optimize for highest accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Optuna optimization with TPE sampler\n",
    "study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study.optimize(objective, n_trials=150)  \n",
    "\n",
    "# Best parameters found by Optuna\n",
    "best_xgb_params = study.best_params\n",
    "print(\"\\nBest XGBoost Parameters (Optuna):\", best_xgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the best XGBoost model\n",
    "best_xgb_model = xgb.XGBClassifier(**best_xgb_params, use_label_encoder=False, eval_metric=\"mlogloss\")\n",
    "best_xgb_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "print(\"\\nXGBoost Model Trained Successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_results = evaluate_model(best_xgb_model, X_test_scaled, y_test, model_name=\"XGBoost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained Random Forest model\n",
    "joblib.dump(best_xgb_model, \"best_xgb_model.pkl\")\n",
    "\n",
    "print(\"XGBoost model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Define hyperparameter search space\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 700),\n",
    "        'max_depth': trial.suggest_int('max_depth', 10, 25),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 32),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 32),\n",
    "        'bootstrap': trial.suggest_categorical('bootstrap', [True, False])\n",
    "    }\n",
    "\n",
    "    # Create RF model\n",
    "    model = RandomForestClassifier(n_jobs=-1, **params, random_state=42, class_weight=\"balanced\")\n",
    "\n",
    "    # Perform cross-validation\n",
    "    score = cross_val_score(model, X_train_resampled, y_train_resampled, cv=3, scoring='accuracy').mean()\n",
    "\n",
    "    # Print results for each trial\n",
    "    print(f\"Trial {trial.number}: {params} â†’ Accuracy: {score:.4f}\")\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Optuna with TPE sampler\n",
    "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study.optimize(objective, n_trials=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print best parameters\n",
    "best_rf_params = study.best_params\n",
    "print(\"\\nBest RF Parameters (Optuna):\", best_rf_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest with best Optuna parameters\n",
    "best_rf_model = RandomForestClassifier(**best_rf_params, n_jobs=-1, random_state=42)\n",
    "best_rf_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "print(\"Random Forest Model Trained Successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_results = evaluate_model(best_rf_model, X_test_scaled, y_test, model_name=\"Random Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained Random Forest model\n",
    "joblib.dump(best_rf_model, \"best_rf_model.pkl\")\n",
    "\n",
    "print(\"Random Forest model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 500),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 15),\n",
    "        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.001, 0.1),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 150),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"reg_alpha\": trial.suggest_loguniform(\"reg_alpha\", 1e-5, 1e-1),\n",
    "        \"reg_lambda\": trial.suggest_loguniform(\"reg_lambda\", 1e-5, 1e-1),\n",
    "    }\n",
    "\n",
    "    # Train LightGBM model\n",
    "    model = LGBMClassifier(**params, random_state=42)\n",
    "    scores = cross_val_score(model, X_train_resampled, y_train_resampled, cv=3, scoring='accuracy')\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run Optuna hyperparameter tuning\n",
    "study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study.optimize(objective, n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train best LightGBM model\n",
    "best_lgbm_params = study.best_params\n",
    "lgbm_model = LGBMClassifier(**best_lgbm_params, random_state=42)\n",
    "lgbm_model.fit(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nBest LightGBM Parameters (Optuna):\", best_lgbm_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_results = evaluate_model(best_lgbm_model, X_test_scaled, y_test, model_name=\"LGBM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save trained LightGBM model\n",
    "joblib.dump(lgbm_model, \"best_lgbm_model.pkl\")\n",
    "print(\"LightGBM Model Trained & Saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp_model(input_shape, num_classes, dropout_rate=0.05, l2_strength=1e-5):\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(512, kernel_regularizer=regularizers.l2(l2_strength), input_shape=(input_shape,)),\n",
    "        layers.LayerNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Dropout(dropout_rate),\n",
    "\n",
    "        layers.Dense(256, kernel_regularizer=regularizers.l2(l2_strength)),\n",
    "        layers.LayerNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Dropout(dropout_rate),\n",
    "\n",
    "        layers.Dense(128, kernel_regularizer=regularizers.l2(l2_strength)),\n",
    "        layers.LayerNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Dropout(dropout_rate * 2),  \n",
    "\n",
    "        layers.Dense(64, kernel_regularizer=regularizers.l2(l2_strength)),\n",
    "        layers.LayerNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "\n",
    "        layers.Dense(num_classes),\n",
    "        layers.Lambda(lambda x: x / 1.5),  # Logits scaling for more stable softmax\n",
    "        layers.Activation('softmax')\n",
    "    ])\n",
    "\n",
    "    initial_learning_rate = 0.001  # ðŸ”¥ Start with better learning rate\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate, decay_steps=5000, decay_rate=0.95, staircase=True\n",
    "    )\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = create_mlp_model(X_train.shape[1], num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(np.unique(y))\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "checkpoint_path = \"mlp_checkpoint.keras\"\n",
    "checkpoint = ModelCheckpoint(\n",
    "    \"mlp_checkpoint.keras\", monitor=\"val_loss\", save_best_only=True, save_weights_only=False, verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "best_mlp_model = load_model(\"mlp_checkpoint.keras\", safe_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mlp_model.fit(\n",
    "    X_train_resampled, y_train_resampled,\n",
    "    epochs=100, batch_size=32, validation_split=0.1,  \n",
    "    verbose=1, callbacks=[early_stopping, checkpoint],  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mlp_model = load_model(\"mlp_checkpoint.keras\", safe_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_results = evaluate_model(best_mlp_model, X_test_scaled, y_test, model_name=\"MLP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasMLPWrapper(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, model, epochs=10, batch_size=32):\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.classes_ = None  # Initialize classes_ attribute\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Train the MLP model and store class labels.\"\"\"\n",
    "        self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, verbose=0)\n",
    "        self.classes_ = np.unique(y)  # Store class labels\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return predicted class labels.\"\"\"\n",
    "        return np.argmax(self.model.predict(X), axis=1)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Return probability estimates for each class.\"\"\"\n",
    "        return self.model.predict(X)  # Softmax output\n",
    "\n",
    "\n",
    "mlp_wrapper = KerasMLPWrapper(model=best_mlp_model)\n",
    "\n",
    "stacking_ensemble = StackingClassifier(\n",
    "    estimators=[\n",
    "        (\"xgb\", best_xgb_model),\n",
    "        (\"lgbm\", best_lgbm_model),\n",
    "        (\"rf\", best_rf_model),\n",
    "        (\"mlp\", mlp_wrapper)\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(),\n",
    "    stack_method=\"predict_proba\"\n",
    ")\n",
    "\n",
    "stacking_ensemble.fit(X_train_resampled, y_train_resampled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define base models\n",
    "base_models = [\n",
    "    (\"xgb\", best_xgb_model),\n",
    "    (\"lgbm\", best_lgbm_model),\n",
    "]\n",
    "\n",
    "# Meta-classifier (final layer)\n",
    "meta_model = LogisticRegression(solver=\"lbfgs\", max_iter=1000, class_weight=\"balanced\", random_state=42)\n",
    "\n",
    "# Create stacking ensemble\n",
    "stacking_ensemble = StackingClassifier(\n",
    "    estimators=base_models,\n",
    "    final_estimator=meta_model,\n",
    "    stack_method=\"auto\",  # Uses 'predict_proba' where available\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train stacking model\n",
    "stacking_ensemble.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Evaluate stacking model\n",
    "stacking_results = evaluate_model(stacking_ensemble, X_test_scaled, y_test, model_name=\"Stacking Ensemble\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Evaluates a given model on test data and prints accuracy, precision, recall, and F1-score.\n",
    "    Also plots the confusion matrix.\n",
    "\n",
    "    Parameters:\n",
    "        model: The trained model to evaluate.\n",
    "        X_test: Test feature set.\n",
    "        y_test: True labels for test data.\n",
    "        model_name (str): Name of the model (for labeling plots).\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing all evaluation metrics.\n",
    "    \"\"\"\n",
    "    # âœ… Fix: Check if model returns probabilities and convert to class labels\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_prob = model.predict_proba(X_test)\n",
    "        y_pred = np.argmax(y_prob, axis=1)  # Convert probabilities to class labels\n",
    "    elif isinstance(model, tf.keras.Model):  # If model is an MLP (TensorFlow/Keras)\n",
    "        y_prob = model.predict(X_test)\n",
    "        y_pred = np.argmax(y_prob, axis=1)  # Convert softmax output to class labels\n",
    "    else:\n",
    "        y_pred = model.predict(X_test)  # Direct class prediction (for models like RF & LGBM)\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average=\"weighted\")\n",
    "    recall = recall_score(y_test, y_pred, average=\"weighted\")\n",
    "    f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "\n",
    "    # Print results\n",
    "    print(f\"\\n===== {model_name} Evaluation =====\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision (Weighted): {precision:.4f}\")\n",
    "    print(f\"Recall (Weighted): {recall:.4f}\")\n",
    "    print(f\"F1-score (Weighted): {f1:.4f}\")\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(f\"Confusion Matrix - {model_name}\")\n",
    "    plt.show()\n",
    "\n",
    "    return {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-score\": f1,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "#Load saved models\n",
    "best_xgb_model = joblib.load(\"best_xgb_model.pkl\")\n",
    "best_rf_model = joblib.load(\"best_rf_model.pkl\")\n",
    "best_lgbm_model = joblib.load(\"best_lgbm_model.pkl\")\n",
    "best_mlp_model = load_model(\"mlp_checkpoint.keras\", safe_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_results = evaluate_model(best_xgb_model, X_test_scaled, y_test, model_name=\"XGBoost\")\n",
    "rf_results = evaluate_model(best_rf_model, X_test_scaled, y_test, model_name=\"Random Forest\")\n",
    "lgbm_results = evaluate_model(best_lgbm_model, X_test_scaled, y_test, model_name=\"Stacking Ensemble\")\n",
    "mlp_results = evaluate_model(best_mlp_model, X_test_scaled, y_test, model_name=\"MLP\")\n",
    "stacking_results = evaluate_model(stacking_ensemble, X_test_scaled, y_test, model_name=\"LightGBM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Function to plot top 10 feature importance\n",
    "def plot_top_features(importances, feature_names, title, ax):\n",
    "    sorted_idx = np.argsort(importances)[::-1]  # Sort features by importance (descending)\n",
    "    top_10_idx = sorted_idx[:10]  # Get top 10 feature indices\n",
    "\n",
    "    sns.barplot(x=np.array(feature_names)[top_10_idx], \n",
    "                y=np.array(importances)[top_10_idx], \n",
    "                palette=\"viridis\", ax=ax)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
    "    ax.set_xlabel(\"Feature\")\n",
    "    ax.set_ylabel(\"Importance Score\")\n",
    "    ax.set_title(title)\n",
    "\n",
    "def mlp_permutation_importance(model, X_test, y_test, feature_names, n_repeats=10):\n",
    "    \"\"\"\n",
    "    Custom function to compute feature importance for MLP using permutation importance.\n",
    "    \"\"\"\n",
    "    baseline_acc = accuracy_score(y_test, np.argmax(model.predict(X_test), axis=1))  # Baseline accuracy\n",
    "    feature_importances = np.zeros(X_test.shape[1])\n",
    "\n",
    "    for i in range(X_test.shape[1]):\n",
    "        X_test_permuted = X_test.copy()\n",
    "        np.random.shuffle(X_test_permuted[:, i])  # Shuffle one feature\n",
    "\n",
    "        acc = accuracy_score(y_test, np.argmax(model.predict(X_test_permuted), axis=1))\n",
    "        feature_importances[i] = baseline_acc - acc  # Importance = drop in accuracy\n",
    "\n",
    "    return feature_importances\n",
    "\n",
    "# Compute MLP feature importance\n",
    "mlp_importance = mlp_permutation_importance(best_mlp_model, X_test_scaled, y_test, X_filtered.columns)\n",
    "\n",
    "# Get top 10 features for MLP\n",
    "sorted_idx = np.argsort(mlp_importance)[::-1][:10]\n",
    "top_features = np.array(X_filtered.columns)[sorted_idx]\n",
    "top_importances = mlp_importance[sorted_idx]\n",
    "\n",
    "# Create a 2x2 grid for subplots\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot Top 10 Features for XGBoost\n",
    "plot_top_features(best_xgb_model.feature_importances_, X_filtered.columns, \"Top 10 Feature Importance - XGBoost\", axs[0, 0])\n",
    "\n",
    "# Plot Top 10 Features for Random Forest\n",
    "plot_top_features(best_rf_model.feature_importances_, X_filtered.columns, \"Top 10 Feature Importance - Random Forest\", axs[0, 1])\n",
    "\n",
    "# Plot Top 10 Features for LightGBM\n",
    "plot_top_features(best_lgbm_model.feature_importances_, X_filtered.columns, \"Top 10 Feature Importance - LightGBM\", axs[1, 0])\n",
    "\n",
    "# Plot MLP Feature Importance\n",
    "sns.barplot(x=top_features, y=top_importances, palette=\"viridis\", ax=axs[1, 1])\n",
    "axs[1, 1].set_xticklabels(axs[1, 1].get_xticklabels(), rotation=45)\n",
    "axs[1, 1].set_xlabel(\"Feature\")\n",
    "axs[1, 1].set_ylabel(\"Importance Score (Î” Accuracy)\")\n",
    "axs[1, 1].set_title(\"Top 10 Feature Importance - MLP (Permutation Importance)\")\n",
    "\n",
    "# Adjust layout to avoid overlap\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import tensorflow as tf\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Evaluates a given model on test data and prints accuracy, precision, recall, and F1-score.\n",
    "    Also returns the confusion matrix.\n",
    "\n",
    "    Parameters:\n",
    "        model: The trained model to evaluate.\n",
    "        X_test: Test feature set.\n",
    "        y_test: True labels for test data.\n",
    "        model_name (str): Name of the model (for labeling plots).\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing all evaluation metrics.\n",
    "        confusion_matrix: The confusion matrix for the model.\n",
    "    \"\"\"\n",
    "    # âœ… Fix: Check if model returns probabilities and convert to class labels\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_prob = model.predict_proba(X_test)\n",
    "        y_pred = np.argmax(y_prob, axis=1)  # Convert probabilities to class labels\n",
    "    elif isinstance(model, tf.keras.Model):  # If model is an MLP (TensorFlow/Keras)\n",
    "        y_prob = model.predict(X_test)\n",
    "        y_pred = np.argmax(y_prob, axis=1)  # Convert softmax output to class labels\n",
    "    else:\n",
    "        y_pred = model.predict(X_test)  # Direct class prediction (for models like RF & LGBM)\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average=\"weighted\")\n",
    "    recall = recall_score(y_test, y_pred, average=\"weighted\")\n",
    "    f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "\n",
    "    # Print results\n",
    "    print(f\"\\n===== {model_name} Evaluation =====\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision (Weighted): {precision:.4f}\")\n",
    "    print(f\"Recall (Weighted): {recall:.4f}\")\n",
    "    print(f\"F1-score (Weighted): {f1:.4f}\")\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    return cm, {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-score\": f1,\n",
    "    }\n",
    "\n",
    "# Evaluate all models and collect confusion matrices\n",
    "xgb_cm, xgb_results = evaluate_model(best_xgb_model, X_test_scaled, y_test, model_name=\"XGBoost\")\n",
    "rf_cm, rf_results = evaluate_model(best_rf_model, X_test_scaled, y_test, model_name=\"Random Forest\")\n",
    "lgbm_cm, lgbm_results = evaluate_model(best_lgbm_model, X_test_scaled, y_test, model_name=\"LightGBM\")\n",
    "mlp_cm, mlp_results = evaluate_model(best_mlp_model, X_test_scaled, y_test, model_name=\"MLP\")\n",
    "stacking_cm, stacking_results = evaluate_model(stacking_ensemble, X_test_scaled, y_test, model_name=\"Stacking Ensemble\")\n",
    "\n",
    "# Create a 2x3 grid for the confusion matrices\n",
    "fig, axs = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Plot each confusion matrix in the grid\n",
    "sns.heatmap(xgb_cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=axs[0, 0])\n",
    "axs[0, 0].set_xlabel(\"Predicted\")\n",
    "axs[0, 0].set_ylabel(\"Actual\")\n",
    "axs[0, 0].set_title(\"Confusion Matrix - XGBoost\")\n",
    "\n",
    "sns.heatmap(rf_cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=axs[0, 1])\n",
    "axs[0, 1].set_xlabel(\"Predicted\")\n",
    "axs[0, 1].set_ylabel(\"Actual\")\n",
    "axs[0, 1].set_title(\"Confusion Matrix - Random Forest\")\n",
    "\n",
    "sns.heatmap(lgbm_cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=axs[0, 2])\n",
    "axs[0, 2].set_xlabel(\"Predicted\")\n",
    "axs[0, 2].set_ylabel(\"Actual\")\n",
    "axs[0, 2].set_title(\"Confusion Matrix - LightGBM\")\n",
    "\n",
    "sns.heatmap(mlp_cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=axs[1, 0])\n",
    "axs[1, 0].set_xlabel(\"Predicted\")\n",
    "axs[1, 0].set_ylabel(\"Actual\")\n",
    "axs[1, 0].set_title(\"Confusion Matrix - MLP\")\n",
    "\n",
    "sns.heatmap(stacking_cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=axs[1, 1])\n",
    "axs[1, 1].set_xlabel(\"Predicted\")\n",
    "axs[1, 1].set_ylabel(\"Actual\")\n",
    "axs[1, 1].set_title(\"Confusion Matrix - Stacking Ensemble\")\n",
    "\n",
    "# Hide the empty subplot\n",
    "axs[1, 2].axis(\"off\")\n",
    "\n",
    "# Adjust layout to avoid overlap\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Store evaluation results\n",
    "model_results = {\n",
    "    \"Model\": [\"XGBoost\", \"Random Forest\", \"LightGBM\", \"MLP\", \"Stacking Ensemble\"],\n",
    "    \"Accuracy\": [0.8776, 0.8705, 0.8790, 0.8509, 0.8797],\n",
    "    \"Precision\": [0.8774, 0.8704, 0.8788, 0.8521, 0.8794],\n",
    "    \"Recall\": [0.8776, 0.8705, 0.8790, 0.8509, 0.8797],\n",
    "    \"F1-score\": [0.8775, 0.8704, 0.8789, 0.8506, 0.8795],\n",
    "}\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "df_results = pd.DataFrame(model_results)\n",
    "\n",
    "# Convert DataFrame to long format for plotting\n",
    "df_long = df_results.melt(id_vars=\"Model\", var_name=\"Metric\", value_name=\"Score\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Store evaluation results\n",
    "model_results = {\n",
    "    \"Model\": [\"XGBoost\", \"Random Forest\", \"LightGBM\", \"MLP\", \"Stacking Ensemble\"],\n",
    "    \"Accuracy\": [0.8776, 0.8705, 0.8790, 0.8509, 0.8797],\n",
    "    \"Precision\": [0.8774, 0.8704, 0.8788, 0.8521, 0.8794],\n",
    "    \"Recall\": [0.8776, 0.8705, 0.8790, 0.8509, 0.8797],\n",
    "    \"F1-score\": [0.8775, 0.8704, 0.8789, 0.8506, 0.8795],\n",
    "}\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "df_results = pd.DataFrame(model_results)\n",
    "\n",
    "# Create four subplots for each metric\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Plot Accuracy\n",
    "sns.lineplot(data=df_results, x=\"Model\", y=\"Accuracy\", marker='o', ax=axes[0, 0])\n",
    "axes[0, 0].set_title(\"Accuracy\")\n",
    "axes[0, 0].set_ylim(0.83, 0.88)  # Adjust y-axis for better clarity\n",
    "\n",
    "# Plot Precision\n",
    "sns.lineplot(data=df_results, x=\"Model\", y=\"Precision\", marker='o', ax=axes[0, 1])\n",
    "axes[0, 1].set_title(\"Precision\")\n",
    "axes[0, 1].set_ylim(0.83, 0.88)  # Adjust y-axis for better clarity\n",
    "\n",
    "# Plot Recall\n",
    "sns.lineplot(data=df_results, x=\"Model\", y=\"Recall\", marker='o', ax=axes[1, 0])\n",
    "axes[1, 0].set_title(\"Recall\")\n",
    "axes[1, 0].set_ylim(0.83, 0.88)  # Adjust y-axis for better clarity\n",
    "\n",
    "# Plot F1-score\n",
    "sns.lineplot(data=df_results, x=\"Model\", y=\"F1-score\", marker='o', ax=axes[1, 1])\n",
    "axes[1, 1].set_title(\"F1-score\")\n",
    "axes[1, 1].set_ylim(0.83, 0.88)  # Adjust y-axis for better clarity\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Set figure size\n",
    "fig, ax = plt.subplots(figsize=(10, 6))  # Reduce width for a more compact look\n",
    "\n",
    "# Define a clear color palette\n",
    "colors = sns.color_palette(\"Set2\", n_colors=len(df_results))\n",
    "\n",
    "# Create bar plot\n",
    "df_results.set_index(\"Model\").plot(kind=\"bar\", color=colors, ax=ax, width=0.7, edgecolor=\"black\")\n",
    "\n",
    "# Add exact values inside the bars, rotated vertically\n",
    "for bars in ax.containers:\n",
    "    ax.bar_label(bars, fmt=\"%.4f\", label_type=\"center\", fontsize=10, color=\"black\", weight=\"bold\", rotation=90, padding=5)\n",
    "\n",
    "# Formatting\n",
    "plt.title(\"Model Performance Comparison\", fontsize=14)\n",
    "plt.ylabel(\"Score\", fontsize=12)\n",
    "plt.xlabel(\"Model\", fontsize=12)\n",
    "plt.xticks(rotation=30, ha=\"right\", fontsize=11)\n",
    "plt.yticks(fontsize=11)\n",
    "\n",
    "# Move legend outside the plot\n",
    "plt.legend(title=\"Metrics\", bbox_to_anchor=(1.05, 1), loc=\"upper left\", fontsize=10)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Compute correlation matrix\n",
    "correlation_matrix = X_filtered.corr()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=False, cmap=\"coolwarm\", linewidths=0.5, center=0)\n",
    "plt.title(\"Feature Correlation Heatmap\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "# Create the Graph object\n",
    "dot = Digraph(comment=\"Machine Learning Workflow\")\n",
    "\n",
    "# Set Graph Attributes (Left to Right + Top to Bottom, Font, Spacing)\n",
    "dot.attr(rankdir=\"TB\", nodesep=\"0.6\", ranksep=\"0.8\", fontname=\"Helvetica\", fontsize=\"12\")\n",
    "\n",
    "# Define Nodes with Color Coding\n",
    "dot.node(\"A\", \"Data Collection\", shape=\"folder\", style=\"filled\", fillcolor=\"lightblue\")\n",
    "dot.node(\"B\", \"Data Preprocessing\", shape=\"box\", style=\"filled\", fillcolor=\"lightblue\")\n",
    "dot.node(\"C\", \"Feature Selection & Scaling\", shape=\"parallelogram\", style=\"filled\", fillcolor=\"lightblue\")\n",
    "dot.node(\"D\", \"Train-Test Split\", shape=\"parallelogram\", style=\"filled\", fillcolor=\"lightblue\")\n",
    "\n",
    "# Hyperparameter Tuning\n",
    "dot.node(\"E1\", \"XGBoost Tuning\", shape=\"hexagon\", style=\"filled\", fillcolor=\"lightgoldenrodyellow\")\n",
    "dot.node(\"E2\", \"LightGBM Tuning\", shape=\"hexagon\", style=\"filled\", fillcolor=\"lightgoldenrodyellow\")\n",
    "dot.node(\"E3\", \"Random Forest Tuning\", shape=\"hexagon\", style=\"filled\", fillcolor=\"lightgoldenrodyellow\")\n",
    "\n",
    "# Model Training\n",
    "dot.node(\"F1\", \"Train XGBoost\", shape=\"ellipse\", style=\"filled\", fillcolor=\"lightgreen\")\n",
    "dot.node(\"F2\", \"Train LightGBM\", shape=\"ellipse\", style=\"filled\", fillcolor=\"lightgreen\")\n",
    "dot.node(\"F3\", \"Train Random Forest\", shape=\"ellipse\", style=\"filled\", fillcolor=\"lightgreen\")\n",
    "dot.node(\"F4\", \"Train MLP Model\", shape=\"ellipse\", style=\"filled\", fillcolor=\"lightseagreen\")\n",
    "\n",
    "# Stacking Ensemble\n",
    "dot.node(\"G\", \"Stacking Ensemble\", shape=\"diamond\", style=\"filled\", fillcolor=\"lightcoral\")\n",
    "\n",
    "# Final Model\n",
    "dot.node(\"H\", \"Final Model\", shape=\"box\", style=\"filled\", fillcolor=\"gold\")\n",
    "\n",
    "# Model Evaluation\n",
    "dot.node(\"I\", \"Model Evaluation\", shape=\"note\", style=\"filled\", fillcolor=\"lightcoral\")\n",
    "dot.node(\"J\", \"Metrics: Accuracy, Precision, Recall, F1\", shape=\"note\", style=\"filled\", fillcolor=\"lightcoral\")\n",
    "\n",
    "# Save Best Model\n",
    "dot.node(\"K\", \"Save Best Models\", shape=\"cylinder\", style=\"filled\", fillcolor=\"lightgray\")\n",
    "\n",
    "# Define Edges (Connections)\n",
    "dot.edge(\"A\", \"B\")\n",
    "dot.edge(\"B\", \"C\")\n",
    "dot.edge(\"C\", \"D\")\n",
    "dot.edge(\"D\", \"F1\")  \n",
    "dot.edge(\"D\", \"F2\")  \n",
    "dot.edge(\"D\", \"F3\")  \n",
    "dot.edge(\"D\", \"F4\")  \n",
    "\n",
    "dot.edge(\"E1\", \"F1\")  # Hyperparameter tuning â†’ Training\n",
    "dot.edge(\"E2\", \"F2\")\n",
    "dot.edge(\"E3\", \"F3\")\n",
    "\n",
    "dot.edge(\"F1\", \"G\")  # Model Training â†’ Stacking\n",
    "dot.edge(\"F2\", \"G\")\n",
    "dot.edge(\"F3\", \"G\")\n",
    "dot.edge(\"F4\", \"G\")  # MLP also goes into Stacking\n",
    "\n",
    "dot.edge(\"G\", \"H\")  # Stacking â†’ Final Model\n",
    "dot.edge(\"H\", \"I\")  # Final Model â†’ Evaluation\n",
    "dot.edge(\"I\", \"J\")  # Evaluation â†’ Metrics\n",
    "dot.edge(\"J\", \"K\")  # Metrics â†’ Save Best Models\n",
    "\n",
    "# Render and Display the Graph\n",
    "dot.render(\"ml_workflow\", format=\"png\", cleanup=True)  # Saves as 'ml_workflow.png'\n",
    "dot.view()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "# Create the Graph object\n",
    "dot = Digraph(comment=\"Malware Impact vs. Classification\")\n",
    "\n",
    "# Set Graph Attributes (Direction, Font, Spacing)\n",
    "dot.attr(rankdir=\"TB\", nodesep=\"0.6\", ranksep=\"0.8\", fontname=\"Helvetica\", fontsize=\"12\")\n",
    "\n",
    "# Malware Impact Path (Red)\n",
    "dot.node(\"A\", \"Malware Enters System\", shape=\"folder\", style=\"filled\", fillcolor=\"lightcoral\")\n",
    "dot.node(\"B\", \"Compromises Files & Data\", shape=\"parallelogram\", style=\"filled\", fillcolor=\"lightcoral\")\n",
    "dot.node(\"C\", \"Spreads & Escalates\", shape=\"parallelogram\", style=\"filled\", fillcolor=\"lightcoral\")\n",
    "dot.node(\"D\", \"Causes Damage: Data Theft, Corruption, Ransomware\", shape=\"box\", style=\"filled\", fillcolor=\"red\")\n",
    "\n",
    "# Classification Path (Blue)\n",
    "dot.node(\"E\", \"Malware Classification\", shape=\"hexagon\", style=\"filled\", fillcolor=\"lightblue\")\n",
    "dot.node(\"F\", \"Identifies Malware Type\", shape=\"ellipse\", style=\"filled\", fillcolor=\"lightblue\")\n",
    "dot.node(\"G\", \"Enables Security Measures\", shape=\"ellipse\", style=\"filled\", fillcolor=\"lightblue\")\n",
    "dot.node(\"H\", \"Prevents & Mitigates Damage\", shape=\"box\", style=\"filled\", fillcolor=\"green\")\n",
    "\n",
    "# Define Edges (Connections)\n",
    "dot.edge(\"A\", \"B\")  # Malware spreading\n",
    "dot.edge(\"B\", \"C\")\n",
    "dot.edge(\"C\", \"D\")  \n",
    "\n",
    "dot.edge(\"A\", \"E\", label=\"Classification Starts\")  # Classification entry point\n",
    "dot.edge(\"E\", \"F\")\n",
    "dot.edge(\"F\", \"G\")\n",
    "dot.edge(\"G\", \"H\")  # Prevention & mitigation\n",
    "\n",
    "# Render and Display the Graph\n",
    "dot.render(\"malware_classification_flowchart\", format=\"png\", cleanup=True)\n",
    "dot.view()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "# Create the Graph object\n",
    "dot = Digraph(comment=\"Stacking Ensemble Architecture\")\n",
    "\n",
    "# Set Graph Attributes (Left to Right for better visualization)\n",
    "dot.attr(rankdir=\"TB\", nodesep=\"0.6\", ranksep=\"0.8\", fontname=\"Helvetica\", fontsize=\"12\")\n",
    "\n",
    "# Input Layer (Feature Set)\n",
    "dot.node(\"A\", \"Input Data (Features)\", shape=\"folder\", style=\"filled\", fillcolor=\"lightblue\")\n",
    "\n",
    "# Base Models\n",
    "dot.node(\"B1\", \"XGBoost\", shape=\"ellipse\", style=\"filled\", fillcolor=\"lightgreen\")\n",
    "dot.node(\"B2\", \"LightGBM\", shape=\"ellipse\", style=\"filled\", fillcolor=\"lightgreen\")\n",
    "dot.node(\"B3\", \"Random Forest\", shape=\"ellipse\", style=\"filled\", fillcolor=\"lightgreen\")\n",
    "dot.node(\"B4\", \"MLP (Neural Network)\", shape=\"ellipse\", style=\"filled\", fillcolor=\"lightseagreen\")\n",
    "\n",
    "# Meta-Classifier\n",
    "dot.node(\"C\", \"Meta-Classifier (Logistic Regression)\", shape=\"diamond\", style=\"filled\", fillcolor=\"gold\")\n",
    "\n",
    "# Final Prediction\n",
    "dot.node(\"D\", \"Final Prediction\", shape=\"box\", style=\"filled\", fillcolor=\"lightgray\")\n",
    "\n",
    "# Define Edges (Connections)\n",
    "dot.edge(\"A\", \"B1\")\n",
    "dot.edge(\"A\", \"B2\")\n",
    "dot.edge(\"A\", \"B3\")\n",
    "dot.edge(\"A\", \"B4\")\n",
    "\n",
    "dot.edge(\"B1\", \"C\")\n",
    "dot.edge(\"B2\", \"C\")\n",
    "dot.edge(\"B3\", \"C\")\n",
    "dot.edge(\"B4\", \"C\")\n",
    "\n",
    "dot.edge(\"C\", \"D\")\n",
    "\n",
    "# Render and Display the Graph\n",
    "dot.render(\"stacking_ensemble_diagram\", format=\"png\", cleanup=True)\n",
    "dot.view()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Collect evaluation results\n",
    "model_results = {\n",
    "    \"Model\": [\"XGBoost\", \"Random Forest\", \"LightGBM\", \"MLP\", \"Stacking Ensemble\"],\n",
    "    \"Accuracy\": [xgb_results[\"Accuracy\"], rf_results[\"Accuracy\"], lgbm_results[\"Accuracy\"], mlp_results[\"Accuracy\"], stacking_results[\"Accuracy\"]],\n",
    "    \"F1-score\": [xgb_results[\"F1-score\"], rf_results[\"F1-score\"], lgbm_results[\"F1-score\"], mlp_results[\"F1-score\"], stacking_results[\"F1-score\"]],\n",
    "    \"Precision\": [xgb_results[\"Precision\"], rf_results[\"Precision\"], lgbm_results[\"Precision\"], mlp_results[\"Precision\"], stacking_results[\"Precision\"]],\n",
    "    \"Recall\": [xgb_results[\"Recall\"], rf_results[\"Recall\"], lgbm_results[\"Recall\"], mlp_results[\"Recall\"], stacking_results[\"Recall\"]],\n",
    "}\n",
    "\n",
    "# Convert dictionary to DataFrame\n",
    "df_results = pd.DataFrame(model_results)\n",
    "\n",
    "# Convert DataFrame to long format for Seaborn boxplot\n",
    "df_long = df_results.melt(id_vars=\"Model\", var_name=\"Metric\", value_name=\"Score\")\n",
    "\n",
    "# Create box plots\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x=\"Metric\", y=\"Score\", data=df_long, hue=\"Model\", palette=\"coolwarm\")\n",
    "\n",
    "# Formatting\n",
    "plt.title(\"Box Plots of Model Performance Metrics\")\n",
    "plt.xlabel(\"Evaluation Metric\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend(title=\"Models\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OSEF-2025-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
